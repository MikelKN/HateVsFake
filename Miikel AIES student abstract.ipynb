{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIAgjUlERCPm6oVlykmuV5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mQX6cVcOb2rA"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Prompt used\n","\n","- Derived following the works of https://aclanthology.org/2022.nlperspectives-1.11/\n","- See Huggingface data and human annotation for creating hatefullness rubric here:\n","  https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech"],"metadata":{"id":"XHbHxF-cb7kW"}},{"cell_type":"code","source":["!pip install openai\n","from openai import OpenAI\n","\n","import pandas as pd\n","import tqdm\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","def emotion_hate(event, model_name= 'gpt-4o-mini'):\n","      prompt = f\"\"\"\n","      You are an expert in online hate speech detection. You will be provided with a text '{event}', your task is to analyze it and assign scores based on the criteria below.\n","      Follow this structured step-by-step approach and judge by the following criteria:\n","      Step 1: Identify the speech type:\n","        DIRECT: The text directly uses hateful language or expresses harmful sentiments\n","        REPORTING: The text quotes, references, or discusses hateful language within an educational, journalistic, or analytical context\n","\n","      Step 2:Assign a hatefulness score based on the criteria that best matches the given text\n","        score 0: Neutral – Content lacking any offensive language, negative sentiment, or harmful implications.\n","        score 1: Mild Insensitivity – Content with potentially insensitive language but lacks clear negative intent or strong negative implications..\n","        score 2: Problematic Content – Content with clearly negative language, dismissive attitudes, or coded offensive phrases that marginalize certain groups or reinforce harmful ideas..\n","        score 3: Targeted Negative Content – Content targets specific identities in a harmful way through stereotypes, indirect attacks, or coded hate.\n","        score 4: Explicitly Hateful – Content with direct slurs, explicit dehumanization, violent threats, incitement to harm, or advocacy of discrimination.\n","\n","      If content contains elements from multiple score levels, focus on the most severe elements present.\n","\n","      Consider these important contextual factors:\n","        The likely intent behind the message\n","        Whether terms are being used self-referentially by in-group members\n","        Whether the content is quoting harmful language to discuss or critique it\n","        Cultural and historical context that may affect interpretation\n","\n","      Step 3: Justify in 1-3 sentences why this score was chosen.\n","\n","      Step 4: Output in this exact format:\n","        speech_type: chosen speech type from Step 1\n","        hate_score: chosen hatefullness score from Step 2\n","        explanation: brief reasoning from Step 3\n","\n","      Ensure that the response strictly follows this format, with no additional explanation, commentary, or justification beyond the required output.\"\"\"\n","\n","      client = OpenAI(\n","          api_key = 'API Key'\n","          )\n","\n","      system_prompt = \"You are an expert in detecting and classifying harmful content online, with specific expertise in recognizing hate speech, implicit bias, and coded language.\"\n","\n","      input = [{\"role\": \"system\", \"content\": system_prompt},\n","              {\"role\": \"user\",\"content\": prompt,}]\n","      chat_completion = client.chat.completions.create(\n","          messages=input,\n","          model=model_name,\n","          temperature=0.1\n","      )\n","      output = chat_completion.choices[0].message.content\n","\n","      print(output)\n","      try:\n","          lines = output.strip().split('\\n')\n","          speech_type_line = next((line for line in iter(lines) if line.lower().startswith('speech_type:')), None)\n","          score_line = next((line for line in iter(lines) if line.lower().startswith('hate_score:')), None)\n","          explanation_line = next((line for line in iter(lines) if line.lower().startswith('explanation:')), None)\n","\n","          if speech_type_line and score_line and explanation_line:\n","              speech_type = speech_type_line.split(':', 1)[1].strip()\n","              score = score_line.split(':', 1)[1].strip()\n","              explanation = explanation_line.split(':', 1)[1].strip()\n","\n","              return {\n","                  \"speech_type\": speech_type,\n","                  \"score\": int(score),\n","                  \"explanation\": explanation\n","              }\n","          else:\n","              return {\n","                  \"speech_type\": None,\n","                  \"score\": None,\n","                  \"explanation\": \"Failed to parse model output\",\n","                  \"raw_output\": output\n","              }\n","      except (IndexError, ValueError) as e:\n","          print(f\"Error processing response: {output} | {e}\")\n","          return {\"speech_type\": \"unknown\",\"score\": \"unknown\", \"explanation\": \"No valid explanation provided\"}\n","\n","\n","df = pd.read_parquet(\".../AIES/WELFAKE/welfake_sample_dataset.csv\")\n","result_df = dataset['text'].apply(lambda x: pd.Series(emotion_hate(x)))\n","dataset['speech_type'] = result_df['speech_type']\n","dataset['score'] = result_df['score']\n","dataset['explanation'] = result_df['explanation']\n","\n","\n"],"metadata":{"id":"Mk8gU-CjvmrE"},"execution_count":null,"outputs":[]}]}